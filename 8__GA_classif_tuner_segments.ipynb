{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8__GA_classif_tuner_segments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1CCxHJL_DI1pr17x73mjgDs2ynfrxNK7f",
      "authorship_tag": "ABX9TyPFPg6A3ncAfOU73iJJKAuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelemiko1/genetic_music_CNN/blob/main/8__GA_classif_tuner_segments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters tuner using Genetic Algorithms"
      ],
      "metadata": {
        "id": "Qt55SGZcJOAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import json\n",
        "import itertools"
      ],
      "metadata": {
        "id": "h5J1n5RSJIK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# const values\n",
        "DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/preprocessed_data_segments.json'\n",
        "INPUT_SHAPE = (39, 130, 1)\n",
        "\n",
        "NUM_GENERATIONS = 15\n",
        "POPULATION = 10\n",
        "PROB_CROSSOVER = 0.8\n",
        "PROB_MUTATION = 0.2"
      ],
      "metadata": {
        "id": "kQ1NLq93JaVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from json file\n",
        "\n",
        "def load_data(dataset_path):\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    X_mfcc = np.array(data['MFCCs'])\n",
        "    y = np.array(data['labels'])\n",
        "    mapping = data['mapping']\n",
        "\n",
        "    return X_mfcc, y, mapping"
      ],
      "metadata": {
        "id": "daA90JBNp1lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to import the dataset\n",
        "\n",
        "def split_dataset_segments(X, y):\n",
        "  \n",
        "    # all genres has 100 songs, but jazz has only 99 (1 was corrupted)\n",
        "    max_songs_per_genre = 100 \n",
        "\n",
        "    # 9 segments has been extracted for each song\n",
        "    max_segments_per_genre = max_songs_per_genre * 9  # 900\n",
        "\n",
        "    # keep 20% of samples for test set\n",
        "    segments_test_per_genre = int(max_segments_per_genre/5)  # 180\n",
        "\n",
        "    # IMPORTANT: I want to avoid to have segments of the same song divided into \n",
        "    # train and test, would be like cheating\n",
        "\n",
        "    # save (almost) equal number of inputs for each genre in test set\n",
        "\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    # consider all the labels \n",
        "    for i in range(10):\n",
        "      \n",
        "      count = 0\n",
        "\n",
        "      # go through all the indexes\n",
        "      for j in range(len(y)):\n",
        "      \n",
        "        # if the segment has the desired label and the number of segments is not reached\n",
        "        if y[j] == i:\n",
        "\n",
        "          if count < segments_test_per_genre:\n",
        "            X_test.append(X[j])\n",
        "            y_test.append(y[j])\n",
        "            # print(f\"label n: {i}, segment n: {count+1} appended to test set\")\n",
        "            count += 1\n",
        "          else:\n",
        "            X_train.append(X[j])\n",
        "            y_train.append(y[j])\n",
        "            # print(f\"label n: {i}, segment n: DON'T KNOW appended to train set\")\n",
        "\n",
        "    # use np arrays\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "  \n",
        "    # add the channels dimension\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "metadata": {
        "id": "yE7DAq1GJkPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fuction to build the CNN model\n",
        "\n",
        "def build_model(parameters, input_shape):\n",
        "\n",
        "    '''\n",
        "    # parameters values\n",
        "    \n",
        "    parameters[0] = number of Conv2D layers added (1,2,3)\n",
        "    parameters[1] = number of Dense layers added  (1,2,3)\n",
        "    parameters[2] = filters 1st Conv2D (min 32, max 128, step 16)\n",
        "    parameters[3] = kernel  1st Conv2D(x) (3, 4, 5)\n",
        "    parameters[4] = kernel  1st Conv2D(y) (3, 4, 5)\n",
        "    parameters[5] = stride  1st Pool2D(x) (2, 3)\n",
        "    parameters[6] = stride  1st Pool2D(y) (2, 3)\n",
        "\n",
        "    parameters[7] = filters 2nd Conv2D (min 32, max 64, step 16)\n",
        "    parameters[8] = filters 3rd Conv2D (min 32, max 64, step 16)\n",
        "    parameters[9] = filters 4th Conv2D (min 32, max 64, step 16)\n",
        "    parameters[10] = neurons 2nd Dense  (min 16, max 128, step 16)\n",
        "    parameters[11] = neurons 3rd Dense  (min 16, max 128, step 16)\n",
        "    parameters[12] = neurons 4th Dense  (min 16, max 128, step 16)\n",
        "    parameters[13] = Dropout probability (0.1 - 0.5)\n",
        "\n",
        "    '''\n",
        "\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # fist conv layer, max pooling and batch normalization\n",
        "\n",
        "    model.add(layers.Conv2D(parameters[2], kernel_size=(parameters[3],parameters[4]),\n",
        "                            input_shape=input_shape, activation='relu'))     \n",
        "       \n",
        "    model.add(layers.MaxPool2D(pool_size=(parameters[5], parameters[6]), padding='same')) \n",
        "\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    # add other Conv layers max pooling and batch normalization\n",
        "\n",
        "    for i in range(parameters[0]):\n",
        "        model.add(layers.Conv2D(parameters[i+7], kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "        \n",
        "        model.add(keras.layers.MaxPool2D((2, 2), strides=(2, 2), padding='same'))\n",
        "        \n",
        "        model.add(layers.BatchNormalization())\n",
        "\n",
        "\n",
        "    # add flatten layer, dense layer and output (dense) layer\n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    # add Dense layers\n",
        "    for i in range(parameters[1]):\n",
        "        model.add(layers.Dense(units=parameters[i+10], activation='relu'))\n",
        "        model.add(layers.Dropout(parameters[13]))\n",
        "\n",
        "    # output layer\n",
        "    model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dl2Wa3cDJqwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform the fitness of a single individual (CNN architecture)\n",
        "\n",
        "def fitness_evaluate(individual, dataset):\n",
        "\n",
        "    # show the individual\n",
        "    print(f\"\\nIndividual considered: {individual}\")\n",
        "\n",
        "    # import dataset\n",
        "    (X_train, y_train), (X_test, y_test) = dataset\n",
        "\n",
        "    # find the input shape of our data. We cas simply use INPUT_SHAPE\n",
        "    input_shape = np.shape(X_train[1])\n",
        "\n",
        "    # create the model\n",
        "    model = build_model(individual, input_shape)\n",
        "\n",
        "    # train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data = (X_test, y_test))\n",
        "\n",
        "    # get the accuracy on test set\n",
        "    print(\"\\n\")\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "TAxkZICgKAZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # inizialize the fist population\n",
        "\n",
        "def inizialize_population(pop_size=5):\n",
        "  \n",
        "\n",
        "    population = []\n",
        "\n",
        "    # for each individual choose a random value in a specified range\n",
        "    for i in range(pop_size):\n",
        "\n",
        "      a = random.randint(1, 3)\n",
        "      b = random.randint(1, 3)\n",
        "      c = 16 * random.randint(2, 8)\n",
        "      d = random.randint(3, 5)\n",
        "      e = random.randint(3, 5)\n",
        "      f = random.randint(2, 3)\n",
        "      g = random.randint(2, 3)\n",
        "\n",
        "      h = 16 * random.randint(2, 4)\n",
        "      i = 16 * random.randint(2, 4)\n",
        "      j = 16 * random.randint(2, 4)\n",
        "      k = 16 * random.randint(1, 8)\n",
        "      l = 16 * random.randint(1, 8)\n",
        "      m = 16 * random.randint(1, 8)\n",
        "\n",
        "      n = 0.1 + random.random() * 4 / 10  # number between 0.1 and 0.4\n",
        "      \n",
        "      individual = [a, b, c, d, e, f, g, h, i, j, k, l, m, n]\n",
        "\n",
        "      # add the ith individual to the population\n",
        "      population.append(individual)\n",
        "\n",
        "    return population"
      ],
      "metadata": {
        "id": "JOfJvfcRKhDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the entire population and save results in a dictionary\n",
        "\n",
        "def population_evaluation(population, dataset):\n",
        "\n",
        "    # dictionary to store the fitness\n",
        "    population_eval = {\n",
        "        \"population\": [],   # [ [80, 5, 3, 48, 128], [128, 5, 3, 128, 112], ... ]\n",
        "        \"fitness\": [],      # [ 0.77, 0.85, 0.54, ... ]\n",
        "        \"probability\": []   # [ 0.21, 0.12, 0.51, ... ]\n",
        "    }\n",
        "    # save population into population_eval\n",
        "    population_eval[\"population\"] = population.copy()\n",
        "\n",
        "    # for each individual (es: [2,4,67,1,2]) find the fitness and save it\n",
        "    for individual in population:\n",
        "\n",
        "        fitness = fitness_evaluate(individual, dataset)\n",
        "        population_eval[\"fitness\"].append(fitness)\n",
        "\n",
        "    # calculate the probability of selection\n",
        "    for fitness in population_eval[\"fitness\"]:\n",
        "\n",
        "        probability = fitness / np.sum(population_eval[\"fitness\"])\n",
        "        population_eval[\"probability\"].append(probability)\n",
        "\n",
        "    return population_eval"
      ],
      "metadata": {
        "id": "wirKR6E_LAT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select two individuals to perform crossover\n",
        "\n",
        "def selection(population_eval):\n",
        "\n",
        "    population_size = len(population_eval[\"fitness\"])\n",
        "    selected_individual = []\n",
        "    selected_individual2 = []\n",
        "\n",
        "    # choose a random individual based on its probability\n",
        "    condition = True\n",
        "    while condition:\n",
        "\n",
        "        # select a random value\n",
        "        random_value = random.random()\n",
        "\n",
        "        # select a random individual\n",
        "        random_index = random.randint(0, population_size - 1)\n",
        "        random_individual = population_eval[\"population\"][random_index]\n",
        "\n",
        "        # if his probability is major than a random value keep it\n",
        "        prob_random_individual = population_eval[\"probability\"][random_index]\n",
        "\n",
        "        if prob_random_individual >= random_value:\n",
        "            selected_individual = random_individual\n",
        "            used_index = random_index\n",
        "            condition = False\n",
        "\n",
        "    # select another individual\n",
        "    condition = True\n",
        "    while condition:\n",
        "\n",
        "        random_value = random.random()\n",
        "        random_index = random.randint(0, population_size - 1)\n",
        "\n",
        "        while random_index == used_index:\n",
        "            random_index = random.randint(0, population_size - 1)\n",
        "\n",
        "        random_individual = population_eval[\"population\"][random_index]\n",
        "        prob_random_individual = population_eval[\"probability\"][random_index]\n",
        "\n",
        "        if prob_random_individual >= random_value:\n",
        "            selected_individual2 = random_individual\n",
        "            condition = False\n",
        "\n",
        "    return selected_individual, selected_individual2"
      ],
      "metadata": {
        "id": "H0WkooZEL-mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crossover function between two individuals\n",
        "\n",
        "def crossover(individual1, individual2, cross_probability=0.8):\n",
        "\n",
        "    # define a crossover position\n",
        "    len_individual = len(individual1)\n",
        "    cross_position = random.randint(1, len_individual-1)\n",
        "\n",
        "    # create the crossed individual\n",
        "    cross_individual = individual1.copy()\n",
        "    cross_individual[cross_position:] = individual2[cross_position:].copy()\n",
        "\n",
        "    # use crossover probability\n",
        "    if random.random() < cross_probability:\n",
        "        return cross_individual\n",
        "    else:\n",
        "        return individual1.copy()"
      ],
      "metadata": {
        "id": "AQruPGc3M6Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mutate_single_value(value, min, max, step):\n",
        "\n",
        "  new_value = value + step * random.randint(-1, 1)\n",
        "\n",
        "  # if it exceeds the boundaries approximate to the limit\n",
        "  if new_value > max:\n",
        "    new_value = max\n",
        "\n",
        "  if new_value < min:\n",
        "    new_value = min\n",
        "\n",
        "  return new_value\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsfPlrL5xl2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mutation function \n",
        "\n",
        "def mutation(individual, prob_mutation=0.2):\n",
        " \n",
        "    mut_individual = individual.copy()\n",
        "\n",
        "    # apply the same probability of mutation for each value\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        mut_individual[0] = random.randint(1, 3)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        mut_individual[1] = random.randint(1, 3)\n",
        "    \n",
        "    if random.random() < prob_mutation:\n",
        "\n",
        "        # perform a random perturbation. If it exceeds give the max or min value\n",
        "        old_value = mut_individual[2]\n",
        "        mut_individual[2] = mutate_single_value(old_value, 32, 128, 16)\n",
        "\n",
        "    \n",
        "    if random.random() < prob_mutation:\n",
        "        mut_individual[3] = random.randint(3, 5)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        mut_individual[4] = random.randint(3, 5)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        mut_individual[5] = random.randint(2, 3)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        mut_individual[6] = random.randint(2, 3)\n",
        "\n",
        "\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[7]\n",
        "        mut_individual[7] = mutate_single_value(old_value, 32, 64, 16)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[8]\n",
        "        mut_individual[8] = mutate_single_value(old_value, 32, 64, 16)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[9]\n",
        "        mut_individual[9] = mutate_single_value(old_value, 32, 64, 16)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[10]\n",
        "        mut_individual[10] = mutate_single_value(old_value, 16, 128, 16)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[11]\n",
        "        mut_individual[11] = mutate_single_value(old_value, 16, 128, 16)\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[12]\n",
        "        mut_individual[12] = mutate_single_value(old_value, 16, 128, 16)\n",
        "\n",
        "\n",
        "    if random.random() < prob_mutation:\n",
        "        \n",
        "        old_value = mut_individual[13]\n",
        "\n",
        "        # set variation between -0.2 and 0.2\n",
        "        variation = (random.random() - 0.5) / 2.5\n",
        "        new_value = old_value + variation\n",
        "\n",
        "        # limit the dropout probability to min 0.1 and max 0.5\n",
        "        if new_value < 0.1:\n",
        "          new_value = 0.1\n",
        "        if new_value > 0.5:\n",
        "          new_value = 0.5\n",
        "\n",
        "        mut_individual[13] = new_value\n",
        "\n",
        "    return mut_individual"
      ],
      "metadata": {
        "id": "j2BMbt0TRV7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# program function that performs all the generations\n",
        "\n",
        "def program():\n",
        "\n",
        "    # import dataset\n",
        "    X_mfcc, y, mapping = load_data(DATASET_PATH)\n",
        "\n",
        "    # split dataset into train and test\n",
        "    (X_train, y_train), (X_test, y_test) = split_dataset_segments(X_mfcc, y)\n",
        "    dataset = (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "    # initialize the population\n",
        "    population = inizialize_population(POPULATION)\n",
        "    print(\"first population initialized\\n\")\n",
        "    print(population)\n",
        "\n",
        "    # initialize best global individual and best global fitness\n",
        "    best_fitness_global = 0\n",
        "    best_individual_global = []\n",
        "    fitness_behaviour = []\n",
        "\n",
        "    for gen in range(NUM_GENERATIONS):\n",
        "\n",
        "        # calculate fitness and probabilities\n",
        "        population_eval = population_evaluation(population, dataset)\n",
        "\n",
        "        # save the best value of this generation\n",
        "        best_fitness = np.max(population_eval[\"fitness\"])\n",
        "        best_index = population_eval[\"fitness\"].index(best_fitness)\n",
        "        best_individual = population_eval[\"population\"][best_index]\n",
        "\n",
        "        # keep track of best fitness of each generation\n",
        "        fitness_behaviour.append(best_fitness)\n",
        "\n",
        "        # save best global index and best global individual if we are 1st gen\n",
        "        if gen == 0:\n",
        "            best_fitness_global = best_fitness\n",
        "            best_individual_global = best_individual\n",
        "\n",
        "        # save best index and individual if it is better than previous generation\n",
        "        if best_fitness > best_fitness_global:\n",
        "            best_fitness_global = best_fitness\n",
        "            best_individual_global = best_individual\n",
        "\n",
        "\n",
        "        # CREATE THE NEW POPULATION - make selection, crossover and mutation until reach pop size\n",
        "\n",
        "        if gen < NUM_GENERATIONS-1:\n",
        "      \n",
        "          new_population = []\n",
        "\n",
        "          for i in range(POPULATION):\n",
        "\n",
        "              # make the selection\n",
        "              individual1, individual2 = selection(population_eval)\n",
        "\n",
        "              # make crossover\n",
        "              cross_individual = crossover(individual1, individual2, PROB_CROSSOVER)\n",
        "\n",
        "              # make mutation\n",
        "              new_individual = mutation(cross_individual, PROB_MUTATION)\n",
        "\n",
        "              new_population.append(new_individual)\n",
        "\n",
        "          population = new_population.copy()\n",
        "          print(f\"\\npopulation number {gen + 2} created:\")\n",
        "          print(population)\n",
        "\n",
        "    return best_fitness_global, best_individual_global, fitness_behaviour"
      ],
      "metadata": {
        "id": "VsmpVe3ZRirR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  fig, axs = plt.subplots(2)\n",
        "\n",
        "  # create the accuracy subplot\n",
        "  axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "  axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "  axs[0].set_ylabel(\"Accuracy\")\n",
        "  axs[0].legend(loc=\"lower right\")\n",
        "  axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "  # create the error subplot\n",
        "  axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "  axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "  axs[1].set_ylabel(\"Error\")\n",
        "  axs[1].set_xlabel(\"Epoch\")\n",
        "  axs[1].legend(loc=\"upper right\")\n",
        "  axs[1].set_title(\"Error eval\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Tzw7s1xNahH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def display_MFCCS(data, number_of_segments=999*9, hop_length=512, sr=22050):\n",
        "    \n",
        "    # select first index or each genre\n",
        "    list_of_indexes = []\n",
        "    for desired_label in range(10):\n",
        "        for temporal_index in range(len(data['labels'])):\n",
        "\n",
        "            #generate a random index between 0 and 999\n",
        "            random_index = int(random.random() * number_of_songs)\n",
        "\n",
        "            #verify if the corresponding label is equal to desired_label\n",
        "            if data['labels'][random_index] == desired_label:\n",
        "                list_of_indexes.append(random_index)\n",
        "                break\n",
        "\n",
        "\n",
        "    # mapping the labels\n",
        "    labels = data['mapping']\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    for i, index in enumerate(list_of_indexes):\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        \n",
        "        #select data\n",
        "        data_array = np.array(data['MFCCs'][index])\n",
        "        image_to_display = data_array\n",
        "\n",
        "        # display MFCCs\n",
        "        librosa.display.specshow(image_to_display, sr=SAMPLE_RATE, hop_length=hop_length)\n",
        "\n",
        "        # extract and print the associated label\n",
        "        current_label_index = data['labels'][index]\n",
        "        current_label_name = labels[current_label_index]\n",
        "        plt.xlabel(f\"{current_label_name}\\n ( sample:{index} )\")\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "wiTG26baF6_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function used to plot the confusion matrix in a convenient way\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "X3UBgPEe24R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform classification with the best CNN architecture obtained\n",
        "\n",
        "def classification(individual, plot_conf_matrix=False):\n",
        "\n",
        "  # build the model\n",
        "  model = build_model(individual, INPUT_SHAPE)\n",
        "\n",
        "  # import dataset\n",
        "  X_mfcc, y, mapping = load_data(DATASET_PATH)\n",
        "\n",
        "  # split dataset into train and test\n",
        "  (X_train, y_train), (X_test, y_test) = split_dataset_segments(X_mfcc, y)\n",
        "\n",
        "  # train the model\n",
        "  history = model.fit(X_train, y_train, batch_size=16, epochs=15, validation_data=(X_test, y_test))\n",
        "\n",
        "  # plot the results\n",
        "  plot_history(history)\n",
        "\n",
        "  # perform evaluation on test set\n",
        "  loss, accuracy = model.evaluate(X_test, y_test)\n",
        "  print(f\"final model loss: {loss}, val_accuracy: {accuracy}\")\n",
        "\n",
        "  # plot confusion matrix\n",
        "  if plot_conf_matrix:\n",
        "\n",
        "    # get the predicted output and obtain the associated label\n",
        "    predictions = model.predict(x = X_test, verbose=0)\n",
        "    rounded_predictions = np.argmax(predictions, axis=-1)\n",
        "    \n",
        "    # compute the confusion matrix, define labels and plot\n",
        "    conf_matrix = confusion_matrix(y_true=y_test, y_pred=rounded_predictions)\n",
        "    cm_plot_labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "    plot_confusion_matrix(cm = conf_matrix, classes = cm_plot_labels, title='Confusion Matrix')\n",
        "\n",
        "  return accuracy\n",
        "  "
      ],
      "metadata": {
        "id": "UeKLrbIvZY4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the program\n",
        "best_fitness_global, best_individual_global, fitness_behaviour = program()\n",
        "\n",
        "# print best results\n",
        "print(f\"\\n BEST FITNESS: {best_fitness_global}\")\n",
        "print(f\"\\n BEST INDIVIDUAL: {best_individual_global}\")\n",
        "print(f\"\\n best fitness for each population: {fitness_behaviour}\")\n"
      ],
      "metadata": {
        "id": "7Lmf-GgbTY5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform classification with best architecture ( ex: [3, 1, 112, 3, 5, 3, 2, 64, 64, 32, 64, 48, 80, 0.3741678893274713] )\n",
        "print(best_individual_global)\n",
        "\n",
        "# number of simulations considered\n",
        "num_test = 3\n",
        "\n",
        "# compute accuracy on test set and save it\n",
        "test_accuracy = []\n",
        "for i in range(num_test):\n",
        "  accuracy = classification(best_individual_global, plot_conf_matrix=True)\n",
        "  test_accuracy.append(accuracy)\n",
        "\n",
        "# get the mean value for the accuracy\n",
        "mean_accuracy_value = np.mean(test_accuracy)\n",
        "print(f\"the test accuracy is: {mean_accuracy_value}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WgL5aL4mbxWS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}